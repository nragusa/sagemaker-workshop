{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Amazon SageMaker with Public Datasets\n",
    "\n",
    "__*Clustering Gene Variants into Geographic Populations*__\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Amazon SageMaker allows you to bring powerful machine learning workflows to data that is already in the cloud.  In this example, we will do just that - combining Amazon SageMaker with data from the [1000 Genomes Project] which is hosted by AWS as a [public dataset].  Specifically, we will perform unsupervised learning using Amazon SageMaker's KMeans algorithm to see if we can predict the geographic population for a set of single nucleotide polymorphisms.\n",
    "\n",
    "Single nucleotide polymorphisms or SNPs (pronounced \"snips\") are single base-pair changes to DNA.  DNA is a long chain molecule that is used to store the \"source code\" for all living organisms and is \"read\" as a sequence of four nucleotides: A, T, C, and G.  A single letter is called a \"base\".  SNPs occur when one of these bases in the sequence changes due to environmental causes or random replication errors during cell division in germ cells (eggs and sperm).  Sometimes these changes are harmless, and sometimes they can cause serious diseases.\n",
    "\n",
    "Here we are going to cluster high frequency SNPs found on Chromosome 6\n",
    "\n",
    "### Attribution\n",
    "This notebook is based on work previously described by [Databricks using Spark][databricks blog]\n",
    "\n",
    "[1000 Genomes Project]: https://aws.amazon.com/1000genomes/\n",
    "[public dataset]: https://aws.amazon.com/public-datasets/\n",
    "[databricks blog]: https://databricks.com/blog/2016/05/24/predicting-geographic-population-using-genome-variants-and-k-means.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "> This notebook was created and tested on an `ml.m4.2xlarge` notebook instance\n",
    "\n",
    "Let's start by:\n",
    "\n",
    "1. Downloading the data we need from S3\n",
    "1. Installing some utility packages for processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data sources\n",
    "We can get variant call data (which describes SNPs, and other kinds of DNA sequence modifications) from the publicly hosted 1000 Genomes dataset on AWS.  We are need the \"\\*.vcf\" file corresponding to Chromosome 6 from the 20130502 release of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws s3 ls --human-readable s3://1000genomes/release/20130502/ | grep chr6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for Chromosome 6 is nearly 1GB in size.  For the purpose of this exercise and to be conservative of space (the scratch area for sagemaker notebooks only have about 5GB of space) we are going to use a sub-sample of the data.  To generate that, we can use `tabix` a bioinformatics command line utility found in the [htslib] set of tools.\n",
    "\n",
    "[htslib]: https://github.com/samtools/htslib\n",
    "\n",
    "The current version of `tabix` (1.8) has been containerized and is hosted on Amazon ECR.  We'll pull down the docker container image into our SageMaker environment and use it to sample the Chromosome 6 VCF file __*directly on S3*__ and create a data file we can use here for model training.  Here we've reduced the data to entries found between positions 1000000-1250000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# get an authentication token for the ECR registry\n",
    "$(aws --region us-west-2 ecr get-login --registry-ids 733263974272 --no-include-email)\n",
    "\n",
    "# run the container\n",
    "docker run --rm -i \\\n",
    "    733263974272.dkr.ecr.us-west-2.amazonaws.com/htslib:latest \\\n",
    "    tabix -h \\\n",
    "    s3://1000genomes/release/20130502/ALL.chr6.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz \\\n",
    "    6:1000000-1250000 > 6-sample.vcf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above doesn't work, you can always just build and run the container locally using the next two cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "FROM centos:latest as builder\n",
    "\n",
    "RUN yum install -y \\\n",
    "  libcurl libcurl-devel \\\n",
    "  openssl-devel \\\n",
    "  gcc \\\n",
    "  bzip2 bzip2-devel \\\n",
    "  xz xz-devel \\\n",
    "  make\n",
    "\n",
    "ENV HTSLIBVER 1.8\n",
    "\n",
    "RUN mkdir -p /opt/samtools \\\n",
    "  && cd /opt/samtools \\\n",
    "  && curl -LO https://github.com/samtools/htslib/releases/download/${HTSLIBVER}/htslib-${HTSLIBVER}.tar.bz2 \\\n",
    "  && tar -xjf htslib*.tar.bz2 \\\n",
    "  && cd htslib-* \\\n",
    "  && ./configure \\\n",
    "  && make\n",
    "\n",
    "\n",
    "FROM centos:latest\n",
    "\n",
    "RUN yum clean all \\\n",
    "  && rm -rf /var/cache/yum\n",
    "\n",
    "WORKDIR /usr/local/bin/\n",
    "COPY --from=builder /opt/samtools/htslib-*/bgzip .\n",
    "COPY --from=builder /opt/samtools/htslib-*/htsfile .\n",
    "COPY --from=builder /opt/samtools/htslib-*/tabix .\n",
    "WORKDIR /root/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker build -t htslib:latest .\n",
    "\n",
    "docker run --rm -i \\\n",
    "    htslib:latest \\\n",
    "    tabix -h \\\n",
    "    s3://1000genomes/release/20130502/ALL.chr6.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz \\\n",
    "    6:1000000-1250000 > 6-sample.vcf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's grab metadata - information about geographic locations of where sample sequences came from - to use as labels in our model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "aws s3 cp s3://1000genomes/release/20130502/integrated_call_samples_v3.20130502.ALL.panel ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "\n",
    "To make exploring and processing the data a little easier, we'll use the `scikit-allel` package.  While this package does not come included with the SageMaker environment, it is easy to install.\n",
    "\n",
    "More information about it can be found at:\n",
    "http://scikit-allel.readthedocs.io/en/latest/index.html\n",
    "\n",
    "It has good utilities for reading VCF files\n",
    "http://alimanfoo.github.io/2017/06/14/read-vcf.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge -y scikit-allel==1.1.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily read in our sampled VCF file for data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import allel\n",
    "\n",
    "callset = allel.read_vcf('6-sample.vcf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above returns a Python dictionary with keys that represent parts of the data.  These are specific parts of the VCF file that are useful for analysis.  For example the `calldata/GT` key contains and array of all the genotype calls for each variant for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callset.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many variants and samples are in this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Samples: {samples}, Variants: {variants}\".format(\n",
    "    variants=len(callset['calldata/GT']), \n",
    "    samples=len(callset['samples']))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data comes from human genome sequences.  Humans have 23 pairs of chromosomes, and hence are \"diploid\" - meaning they should have two copies of any given DNA sequence (with a couple exceptions - e.g. genes in the XY chromosomes).\n",
    "\n",
    "A variant in a copy of a DNA sequence is called an \"allele\".  At minimum, there is at least one allele - the DNA sequence that matches the human reference genome.  Alleles that do not match the reference are called \"alternates\".\n",
    "\n",
    "There appear to be up to 6 alternate alleles for each variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.unique(callset['calldata/GT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A genotype is a combination of variants for a DNA sequence position, over all copies.  For example, let's say that the reference for a DNA position is 'A', and a variant for the position is 'T'.  The possible genotypes for this position would be:\n",
    "\n",
    "* REF / REF - \"homozygous\" for the reference\n",
    "* REF / ALT - \"heterozygous\"\n",
    "* ALT / REF - \"heterozygous\"\n",
    "* ALT / ALT - \"homozygous\" for the alternate\n",
    "\n",
    "Typical genotype calls use integer IDs to represent the REF and ALT alleles, with REF always being '0'.  Alternative alleles start at '1' and count up to the total number of alternative alleles for the variant.  So the possible genotypes for the example above would be:\n",
    "\n",
    "* 0 / 0\n",
    "* 0 / 1\n",
    "* 1 / 0\n",
    "* 1 / 1\n",
    "\n",
    "In cases where there is more than one alternate allele, you might see genotypes like '0 / 2' or '1 / 2'.\n",
    "\n",
    "All the genotypes in the data can be collected in a `GenotypeArray` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gt = allel.GenotypeArray(callset['calldata/GT'])\n",
    "gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes sequence isn't perfect - base calling can produce ambiguous results.  This results in missing variant calls and incomplete genotypes.  For any machine learning task, it is important to know if there is missing data and deal with it accordingly.\n",
    "\n",
    "The `GenotypeArray` also tells us that there are no missing calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.count_missing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, variants are a combination of SNPs, InDels, and Copy Number variants (duplications of DNA, beyond the chromosome count).  We can see that in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(callset['variants/ALT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Modeling Problem\n",
    "\n",
    "### Feature selection\n",
    "For this modeling exercise, we are going to use the variant \"ID\" - combination of the chromosome position, the reference allele, and the alternative allele as features for K-Means clustering.  Above we saw that there were about 8300 variants.  We want to reduce this down to a more manageable set, which can improve our clustering performance.\n",
    "\n",
    "To start, we'll just focus on SNPs that are bi-allelic variants.  That is a SNP with only one alternate allele.  To do this, we filter for entries where there is only 1 nucleotide in the REF and ALT lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 8090 variants with single nucleotide reference alleles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REF = callset['variants/REF']\n",
    "REF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "is_1bp_ref = np.array(list(map(lambda r: len(r) == 1, REF)), dtype=bool)\n",
    "sum(is_1bp_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 8147 variants with single nucleotide alternate alleles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the default import considers only 3 allele alternatives\n",
    "ALT = callset['variants/ALT']\n",
    "ALT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "is_1bp_alt = np.array(list(map(lambda a: len(a[0]) == 1 and not any(a[1:]), ALT)), dtype=bool)\n",
    "sum(is_1bp_alt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intersection of the above yields 7946 SNP variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_snp_var = is_1bp_ref & is_1bp_alt\n",
    "sum(is_snp_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reduce the feature set further by only considering variants with alternate allele frequencies > 30%.  This will eliminate rare variants that won't help our clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a count of alleles for each variant\n",
    "ac = gt.count_alleles()\n",
    "ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# since we're only looking for only bi-allelic SNPs, we're only concerned with \n",
    "# the reference and first alternative allele\n",
    "is_hifreq_snp_var = is_snp_var & (ac[:, 1] / (ac[:, 0] + ac[:, 1]) > .30)\n",
    "sum(is_hifreq_snp_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now down to ~376 features, which is certainly more manageable than the ~8300 we started with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformation\n",
    "\n",
    "Machine learning algorithms work best on numerical data.\n",
    "\n",
    "We can convert the genotypes into integer values easily using bit-packing provided by the `GenotypeArray.to_packed` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = allel.GenotypeArray([\n",
    "    [[0,0], [0,1], [1,0], [1,1]]\n",
    "])\n",
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx.to_packed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we see that a values of 1 and 16 are effectively equivalent - they correspond to the same genotype: heterozygous for the alt allele.  Where 17 is homozygous for the alt allele.  We can recode these to:\n",
    "* 16 --> 1\n",
    "* 17 --> 2\n",
    "\n",
    "We'll apply this transformation to our GenoTypeArray, which we'll use as our data for training, and apply the filter for only high frequency SNPs that we generated above.  We also want the data entries to list samples along the rows and variants along the columns, so we'll use the transpose of the coded GenoTypeArray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_coded = gt.to_packed()[is_hifreq_snp_var,:]\n",
    "gt_coded[gt_coded == 16] = 1\n",
    "gt_coded[gt_coded == 17] = 2\n",
    "gt_coded.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `CHROM`, `POS`, `REF`, and `ALT` fields from the variant data to create variant feature IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    '{}-{}-{}-{}'.format(c, p, r, a[0])\n",
    "    for c, p, r, a in zip(\n",
    "        callset['variants/CHROM'], \n",
    "        callset['variants/POS'], \n",
    "        callset['variants/REF'], \n",
    "        callset['variants/ALT']\n",
    "    )]\n",
    "features = np.array(features)[is_hifreq_snp_var]\n",
    "features[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in the panel metadata to get class labels - the geographic location that each sample originated from.  There are many popluations in the data set.  For this example, we'll only focus on the following populations:\n",
    "\n",
    "* GBR: British from England and Scotland\n",
    "* ASW: African Ancestry in Southwest US\n",
    "* CHB: Han Chinese in Bejing, China\n",
    "\n",
    "Here we'll use `pandas` to process the metadata panel into classes we can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "classes = pd.read_table('integrated_call_samples_v3.20130502.ALL.panel', usecols=['sample', 'pop'])\n",
    "classes = classes[classes['pop'].isin(['GBR', 'ASW', 'CHB'])].copy()\n",
    "classes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the data is distributed across each of our target populations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "classes.groupby('pop').count().reset_index().plot.bar('pop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the chart above, the distribution of samples across these three populations looks reasonable - i.e. each group has roughly the same number of samples.\n",
    "\n",
    "Let's now create the data frame to feed into model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = classes.merge(\n",
    "    pd.concat((\n",
    "        pd.Series(callset['samples'], name='sample'),\n",
    "        pd.DataFrame(gt_coded.transpose(), columns=features)),\n",
    "        axis=1),\n",
    "    on='sample'\n",
    ")\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all of our processing, we have a data set with 255 samples and ~376 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "This dataset is small, only 255 observations, but should give an idea of how to use the built-in KMeans algorithm.\n",
    "\n",
    "Let's use an 80/20 ratio a train/test split.  We'll train the KMeans clustering model with the `train` set and use the `test` set to evaluate predictions.  To prepare for training, we need to remove all non-numeric values, so below we'll drop the `pop` field from the coded genotype data and store it with labels that we can use later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil, floor\n",
    "\n",
    "train_data = data.sample(frac=.8, random_state=1024)\n",
    "test_data = data[~data['sample'].isin(train_data['sample'])].copy()\n",
    "\n",
    "train_labels = train_data[['sample', 'pop']].copy().set_index('sample')\n",
    "train_labels['pop'] = pd.Categorical(train_labels['pop'])\n",
    "train_data = train_data.drop(columns='pop').set_index('sample')\n",
    "\n",
    "test_labels = test_data[['sample', 'pop']].copy().set_index('sample')\n",
    "test_labels['pop'] = pd.Categorical(test_labels['pop'])\n",
    "test_data = test_data.drop(columns='pop').set_index('sample')\n",
    "\n",
    "print('Observations')\n",
    "print(f'training: {train_data.shape[0]}')\n",
    "print(f'test: {test_data.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the standard setup for SageMaker training using KMeans.\n",
    "\n",
    "Be sure to set the `bucket` name to something you have access to.\n",
    "The fitting process will upload the training data to this bucket for the training instance(s) to access.  Once training is done, a model will be uploaded to the bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import KMeans, get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "bucket = '<your_s3_bucket_name_here>'\n",
    "\n",
    "data_location = 's3://{}/sagemaker/genome-kmeans/data'.format(bucket)\n",
    "output_location = 's3://{}/sagemaker/genome-kmeans/output'.format(bucket)\n",
    "\n",
    "print('training data will be uploaded to: {}'.format(data_location))\n",
    "print('training artifacts will be uploaded to: {}'.format(output_location))\n",
    "\n",
    "kmeans = KMeans(role=role,\n",
    "                train_instance_count=2,\n",
    "                train_instance_type='ml.c4.8xlarge',\n",
    "                output_path=output_location,\n",
    "                k=3,\n",
    "                data_location=data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to train the model.  This should take only about 5-9 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "kmeans.fit(kmeans.record_set(np.float32(train_data.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Endpoint Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's deploy the model behind an endpoint we can use for predictions.  This process takes about 5-9 mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "kmeans_predictor = kmeans.deploy(initial_instance_count=1,\n",
    "                                 instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our newly deployed endpoint to test the model.\n",
    "\n",
    "The predictor will return a results object from which we can extract the cluster assignments for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "result = kmeans_predictor.predict(np.float32(train_data))\n",
    "clusters = np.int0([r.label['closest_cluster'].float32_tensor.values[0] for r in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how these predicted clusters map to the real classes\n",
    "\n",
    "First, how well did the training set cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_labels['pop'], columns=clusters, colnames=['cluster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this cross tabulation we see that there are clusters with majority membership in each of our populations.\n",
    "\n",
    "What do the clusters look like visually?  To answer this question, we'll generate a force weighted graph of the clusters and color code them by their original population code.\n",
    "\n",
    "To accomplish this, well use the [lightning-viz](http://lightning-viz.org/) package for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightning-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning import Lightning\n",
    "\n",
    "lgn = Lightning(ipython=True, local=True)\n",
    "\n",
    "graph_data = [\n",
    "    {\n",
    "        'cluster': int(r.label['closest_cluster'].float32_tensor.values[0]),\n",
    "        'distance': float(r.label['distance_to_cluster'].float32_tensor.values[0])\n",
    "    }\n",
    "    for r in result\n",
    "]\n",
    "\n",
    "gg = pd.concat(\n",
    "    (train_labels.reset_index(), \n",
    "     pd.DataFrame(graph_data)),\n",
    "    axis=1\n",
    ")\n",
    "gg['code'] = pd.np.NaN  # place holder for population category codes\n",
    "\n",
    "gg = pd.concat(\n",
    "    (pd.DataFrame({\n",
    "        'cluster': [0,1,2], \n",
    "        'distance': 0, \n",
    "        'sample': ['0', '1', '2'], \n",
    "        'pop': ''}\n",
    "    ), gg)).reset_index().drop(columns='index')\n",
    "gg['code'] = pd.Categorical(gg['pop']).codes\n",
    "\n",
    "\n",
    "# generate the network links and plot\n",
    "nn = [(r[0], r[1], r[2]) for r in gg.to_records()]\n",
    "lgn.force(nn, group=gg['code'], labels=gg['sample'] + '\\n' + gg['pop'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustering results are roughly the same on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = kmeans_predictor.predict(np.float32(test_data))\n",
    "clusters = np.int0([r.label['closest_cluster'].float32_tensor.values[0] for r in result])\n",
    "pd.crosstab(test_labels['pop'], columns=clusters, colnames=['cluster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bottom Line\n",
    "\n",
    "The mixture of populations in the clusters may be interpretted as individuals with mixed ancestry.  Also, the clustering could be improved further if there was additional dimensionality reduction (e.g. via PCA), more samples, or both.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Delete the Endpoint\n",
    "If you're ready to be done with this notebook, make sure run the cell below.  This will remove the hosted endpoint you created and avoid any charges from a stray instance being left on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kmeans_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sagemaker.Session().delete_endpoint(kmeans_predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
